#!/usr/bin/env python3.6

import os
import sys
import fcntl
import logging
import json
import stat
import errno
import pprint
import boto3
import aiohttp
import asyncio
import filelock

import s3
import gg
import gg_pb2

from contextlib import closing
from base64 import b64decode
from ggpaths import GGPaths, GGCache
from common import make_executable, is_executable, sizeof_fmt

CHUNK_SIZE = 32 * 1024 # 32 KB
TIMEOUT = 5
MAX_RETRIES = 3

s3_bucket = os.environ.get('GG_S3_BUCKET')
awscreds = boto3.Session().get_credentials()
aws_auth_generator = s3.S3AuthGeneator(awscreds.access_key, awscreds.secret_key)
lock_directory = "/tmp"

@aiohttp.streamer
def file_sender(writer, file_name=None):
    with open(file_name, 'rb') as f:
        chunk = f.read(CHUNK_SIZE)
        while chunk:
            yield from writer.write(chunk)
            chunk = f.read(CHUNK_SIZE)

async def upload_dependency(session: aiohttp.ClientSession, dep_hash: str):
    upload_lock_filename = os.path.join(lock_directory, "upload-s3_{}".format(dep_hash))
    upload_lock = filelock.FileLock(upload_lock_filename, timeout=0)

    retries = 0

    while True:
        try:
            with upload_lock:
                object_url = s3.get_object_url(s3_bucket, dep_hash)
                headers = {}
                aws_auth_generator('HEAD', s3_bucket, dep_hash, headers)

                async with session.head(object_url, headers=headers, timeout=TIMEOUT) as r:
                    if r.status == 200:
                        return

                filename = GGPaths.blob_path(dep_hash)
                executable = is_executable(filename)

                headers = {
                    'x-amz-acl': 'public-read',
                    'x-amz-tagging': 'gg:executable={}'.format('true' if executable else 'false'),
                    'Content-Length': '%d' % os.path.getsize(filename)
                }

                aws_auth_generator('PUT', s3_bucket, dep_hash, headers)

                async with session.request('PUT', object_url, headers=headers, data=file_sender(filename), timeout=TIMEOUT) as r:
                    assert r.status == 200

                return
        except filelock.Timeout:
            await asyncio.sleep(1)
        except:
            retries += 1
            if retries > MAX_RETRIES:
                raise
            await asyncio.sleep(1)
        finally:
            try:
                os.remove(upload_lock_filename)
            except:
                pass

@asyncio.coroutine
def upload_all_dependencies(dependecies):
    with aiohttp.ClientSession() as session:
        upload_futures = [upload_dependency(session, d['hash']) for d in dependecies]

        for upload_future in asyncio.as_completed(upload_futures):
            result = yield from upload_future

def push_infiles(infiles):
    with closing(asyncio.get_event_loop()) as loop:
       loop.run_until_complete(upload_all_dependencies(infiles))

def main():
    thunk_hash = sys.argv[1]

    if GGCache.check(thunk_hash):
        # already reduced
        return os.EX_OK

    thunk = gg.read_thunk(thunk_hash)

    infiles = [{'hash': i.hash,
                'executable': i.type == gg_pb2.InFile.Type.Value('EXECUTABLE'),
                'size': i.size}
               for i in thunk.infiles
               if i.type != gg_pb2.InFile.Type.Value('DUMMY_DIRECTORY')]

    infiles += [{
        'hash': thunk_hash, 'executable': False,
        'size': os.path.getsize(GGPaths.blob_path(thunk_hash))
    }]

    request_payload = {
        'thunk_hash': thunk_hash,
        's3_bucket': s3_bucket,
        'infiles': infiles
    }

    print('uploading dependecies ({}, {}) for {:.12} '.format(
        len(infiles), sizeof_fmt(sum([x['size'] for x in infiles])), thunk_hash),
          file=sys.stderr)
    push_infiles(infiles)

    response = boto3.client('lambda').invoke(
        FunctionName='ggfunction',
        InvocationType='RequestResponse',
        Payload=json.dumps(request_payload),
        LogType='Tail',
    )

    if response.get('StatusCode', 0) / 100 != 2 or 'FunctionError' in response:
        print(b64decode(response.get('LogResult')).decode('utf-8'), file=sys.stderr)
        raise Exception('Lambda exceution failed.')

    response_payload = json.loads(response['Payload'].read())
    output_hash = response_payload['output_hash']

    GGCache.insert(thunk_hash, output_hash)
    print('\u03bb({:.12}) => {:.12}'.format(thunk_hash, output_hash), file=sys.stderr)

    return os.EX_OK

def usage(argv0):
    print("usage: {} THUNK-HASH".format(argv0), file=sys.stderr)

def init():
    if not s3_bucket:
        raise Exception("GG_S3_BUCKET environment variable not set")

    if len(sys.argv) != 2:
        usage(sys.argv[0])
        sys.exit(os.EX_USAGE)

if __name__ == '__main__':
    try:
        init()
        sys.exit(main())
    except Exception as ex:
        print(ex, file=sys.stderr)
        sys.exit(1)
