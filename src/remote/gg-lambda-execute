#!/usr/bin/env python3.6

import os
import sys
import fcntl
import logging
import json
import stat
import errno
import pprint
import boto3
import aiohttp
import asyncio

import s3
import gg
import gg_pb2

from contextlib import closing
from base64 import b64decode
from ggpaths import GGPaths, GGCache

CHUNK_SIZE = 32 * 1024 # 32 KB

s3_bucket = os.environ.get('GG_S3_BUCKET')
awscreds = boto3.Session().get_credentials()
aws_auth_generator = s3.S3AuthGeneator(awscreds.access_key, awscreds.secret_key)

@aiohttp.streamer
def file_sender(writer, file_name=None):
    with open(file_name, 'rb') as f:
        chunk = f.read(CHUNK_SIZE)
        while chunk:
            yield from writer.write(chunk)
            chunk = f.read(CHUNK_SIZE)

async def upload_dependency(session: aiohttp.ClientSession, dep_hash: str):
    filename = GGPaths.blob_path(dep_hash)

    with open(filename, "a") as _f:
        try:
            fcntl.lockf(_f, fcntl.LOCK_EX | fcntl.LOCK_NB)
        except OSError as e:
            if e.errno in [errno.EACCES, errno.EAGAIN]:
                await asyncio.sleep(1)
            else:
                raise
        except:
            raise

        object_url = s3.get_object_url(s3_bucket, dep_hash)
        headers = {}
        aws_auth_generator('HEAD', s3_bucket, dep_hash, headers)

        async with session.head(object_url, headers=headers) as r:
            if r.status == 200:
                return

        headers = {
            'x-amz-acl': 'public-read',
            'x-amz-tagging': 'gg=file',
            'Content-Length': '%d' % os.path.getsize(filename)
        }

        aws_auth_generator('PUT', s3_bucket, dep_hash, headers)

        async with session.request('PUT', object_url, headers=headers, data=file_sender(filename)) as r:
            assert r.status == 200

@asyncio.coroutine
def upload_all_dependencies(dependecies):
    with aiohttp.ClientSession() as session:
        upload_futures = [upload_dependency(session, d['hash']) for d in dependecies]

        for upload_future in asyncio.as_completed(upload_futures):
            result = yield from upload_future

def push_infiles(infiles):
    with closing(asyncio.get_event_loop()) as loop:
       loop.run_until_complete(upload_all_dependencies(infiles))

def make_executable(path):
    st = os.stat(path)
    os.chmod(path, st.st_mode | stat.S_IEXEC)

def main():
    thunk_hash = sys.argv[1]

    if GGCache.check(thunk_hash):
        # already reduced
        return os.EX_OK

    thunk = gg.read_thunk(thunk_hash)

    infiles = [{'hash': i.hash,
                'executable': i.type == gg_pb2.InFile.Type.Value('EXECUTABLE'),
                'size': i.size}
               for i in thunk.infiles
               if i.type != gg_pb2.InFile.Type.Value('DUMMY_DIRECTORY')]

    infiles += [{
        'hash': thunk_hash, 'executable': False,
        'size': os.path.getsize(GGPaths.blob_path(thunk_hash))
    }]

    request_payload = {
        'thunk_hash': thunk_hash,
        's3_bucket': s3_bucket,
        'infiles': infiles
    }

    print('uploading dependecies ({}) for {:.12}. '.format(len(infiles), thunk_hash),
          file=sys.stderr)
    push_infiles(infiles)

    response = boto3.client('lambda').invoke(
        FunctionName='ggfunction',
        InvocationType='RequestResponse',
        Payload=json.dumps(request_payload),
        LogType='Tail',
    )

    if response.get('StatusCode', 0) / 100 != 2 or 'FunctionError' in response:
        print(b64decode(response.get('LogResult')).decode('utf-8'), file=sys.stderr)
        raise Exception('Lambda exceution failed.')

    response_payload = json.loads(response['Payload'].read())
    output_hash = response_payload['output_hash']

    boto3.client('s3').download_file(s3_bucket, output_hash, GGPaths.blob_path(output_hash))

    if response_payload['executable_output']:
        make_executable(GGPaths.blob_path(output_hash))

    GGCache.insert(thunk_hash, output_hash)
    print('\u03bb({:.12}) => {:.12}'.format(thunk_hash, output_hash), file=sys.stderr)

    return os.EX_OK

def usage(argv0):
    print("usage: {} THUNK-HASH".format(argv0), file=sys.stderr)

def init():
    if not s3_bucket:
        raise Exception("GG_S3_BUCKET environment variable not set")

    if len(sys.argv) != 2:
        usage(sys.argv[0])
        sys.exit(os.EX_USAGE)

if __name__ == '__main__':
    try:
        init()
        sys.exit(main())
    except Exception as ex:
        print(ex, file=sys.stderr)
        sys.exit(1)
